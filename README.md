# DATA-WAREHOUSE-PYSPARK-SQL-SERVER-HADOOP-HIVE


ğŸ—ï¸ Modern Data Warehouse & Analytics Project

 Building a Scalable Data Warehouse using PySpark and the Medallion Architecture (Bronze, Silver & Gold Layers)

 Project Overview

This project demonstrates the end-to-end development of a modern data warehouse using PySpark, designed for analytical reporting and business intelligence.

Starting from SQL Server as the data source, I built a structured ETL pipeline that ingests data into a Bronze layer, processes and transforms it in the Silver layer, and refines it in the Gold layer for business analytics â€” all stored efficiently in Parquet format.

The entire architecture was deployed on a Hadoop ecosystem, leveraging Hive for data warehousing operations and SQL-like querying across distributed storage.

ğŸ”¹ Key Features

âœ” Implementation of the Medallion Architecture (Bronze, Silver, and Gold Layers)

âœ” PySpark-based ETL Pipelines for data extraction, transformation, and loading

âœ” Data stored in Parquet format across all layers for optimized performance

âœ” Integration with Hadoop and Hive for distributed data management and querying

âœ” Star Schema Modeling (Fact & Dimension tables) for efficient analytical queries

âœ” End-to-end data quality processing to standardize and validate datasets


ğŸš€ Project Requirements

1ï¸âƒ£ Data Engineering: Building the Data Warehouse

Objective:

Design and implement a scalable data warehouse using PySpark and the Hadoop ecosystem to consolidate data from multiple sources and enable high-performance analytics.

Key Specifications:

âœ” Data Source: SQL Server (structured sales and CRM data)

âœ” Bronze Layer: Ingest raw data from SQL Server and store in Parquet format

âœ” Silver Layer: Apply data transformation and cleansing using PySpark; save as Parquet

âœ” Gold Layer: Implement business logic and create aggregated datasets in Parquet

âœ” Hive Integration: Enable SQL-based querying and metadata management

âœ” Documentation: Include ER diagrams, schema definitions, and business logic flow



2ï¸âƒ£ Business Intelligence & Analytics: Data Analysis

Objective:

Deliver actionable insights and business metrics through optimized querying and data modeling.

Insights Delivered:

ğŸ“Š Customer Behavior Analysis â€“ Identified key customer segments and purchasing trends

ğŸ“Š Product Performance Evaluation â€“ Assessed profitability and sales performance

ğŸ“Š Sales Trends & Forecasting â€“ Highlighted seasonal patterns to guide business strategy


ğŸ§° Tech Stack & Tools


ğŸ› ï¸ PySpark â€“ ETL and transformation workflows

ğŸ› ï¸ SQL Server â€“ Source system for operational data

ğŸ› ï¸ Hadoop & Hive â€“ Distributed storage and query layer

ğŸ› ï¸ Parquet â€“ Optimized columnar file storage

ğŸ› ï¸ Tableau / Power BI (Optional) â€“ Visualization and reporting

ğŸ› ï¸ GitHub â€“ Version control and project documentation

